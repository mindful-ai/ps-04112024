{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenization is the process of breaking down a text into smaller components, typically words or subwords, called tokens. It’s a crucial step in natural language processing, as it transforms text into a structured format that models can work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\mindf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tokenization', 'is', 'essential', 'of', 'natural', 'language', 'processing']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"Tokenization is essential of natural language processing\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stop Word Removal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stop word removal is the process of eliminating common words (like \"the,\" \"is,\" \"and\") from text data. These words generally carry less meaningful information in natural language processing (NLP) tasks, so removing them can help reduce the noise and focus on the more important terms in the text.\n",
    "\n",
    "Let’s say we are analyzing customer reviews for a product. The sentence \"The product is very good and easy to use\" includes several stop words that don’t add much value in terms of understanding customer sentiment. After removing stop words, we might get: \"product very good easy use,\" which still conveys the main sentiment without unnecessary words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['product', 'good', 'easy', 'use', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\mindf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "text = \"The product is very good and easy to use.\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Get the list of English stop words\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "# Remove stop words\n",
    "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "print(filtered_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization and Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatization and Stemming are both techniques used in natural language processing (NLP) to reduce words to their base forms, but they do so in different ways:\n",
    "\n",
    "Stemming: This is a more aggressive method that removes suffixes from words in an attempt to reduce them to their \"stem\" form, which may not necessarily be a valid word.\n",
    "Lemmatization: This process reduces words to their base or dictionary form (lemma). It considers the word’s meaning and context, so it is more precise than stemming."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s consider the words \"running,\" \"ran,\" and \"runner.\"\n",
    "\n",
    "Stemming might reduce all of these to \"run.\"\n",
    "Lemmatization would reduce \"running\" to \"run,\" but would leave \"runner\" as \"runner\" (since it’s a different base form)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\mindf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\mindf\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Tokens: ['the', 'cat', 'are', 'run', 'faster', 'than', 'the', 'runner', '.']\n",
      "Lemmatized Tokens: ['The', 'cat', 'are', 'run', 'faster', 'than', 'the', 'runners', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# Download necessary resources\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "\n",
    "text = \"The cats are running faster than the runners.\"\n",
    "\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(word) for word in tokens]\n",
    "print(\"Stemmed Tokens:\", stemmed_tokens)\n",
    "\n",
    "# Lemmatization\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(word, pos='v') if word not in ['the', 'are'] else word for word in tokens]\n",
    "print(\"Lemmatized Tokens:\", lemmatized_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine Similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity is a metric used to measure how similar two text documents (or vectors) are, based on the cosine of the angle between them. It’s often used in text analysis and natural language processing (NLP) to determine the similarity between two documents, regardless of their size. The cosine similarity ranges from 0 (completely dissimilar) to 1 (completely similar)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TfidfVectorizer\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpairwise\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m cosine_similarity\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Download necessary resources\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Define the documents\n",
    "doc1 = \"I love programming in Python.\"\n",
    "doc2 = \"Python programming is fun.\"\n",
    "\n",
    "# Create a list of documents\n",
    "documents = [doc1, doc2]\n",
    "\n",
    "# Remove stopwords (for better results)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "def preprocess_text(text):\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    return [word for word in tokens if word.isalnum() and word not in stop_words]\n",
    "\n",
    "# Apply preprocessing to both documents\n",
    "doc1_tokens = preprocess_text(doc1)\n",
    "doc2_tokens = preprocess_text(doc2)\n",
    "\n",
    "# Create the TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents into TF-IDF vectors\n",
    "tfidf_matrix = vectorizer.fit_transform([' '.join(doc1_tokens), ' '.join(doc2_tokens)])\n",
    "\n",
    "# Compute cosine similarity\n",
    "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
    "\n",
    "print(f\"Cosine Similarity between doc1 and doc2: {cosine_sim[0][0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\mindf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mindf\\appdata\\local\\packages\\pythonsoftwarefoundation.python.3.12_qbz5n2kfra8p0\\localcache\\local-packages\\python312\\site-packages (from scikit-learn) (1.4.2)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp312-cp312-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/11.0 MB 4.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.3/11.0 MB 3.2 MB/s eta 0:00:04\n",
      "   ------ --------------------------------- 1.8/11.0 MB 3.0 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/11.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/11.0 MB 3.0 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.9/11.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------- ----------------------- 4.5/11.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------------------ --------------------- 5.0/11.0 MB 3.0 MB/s eta 0:00:03\n",
      "   -------------------- ------------------- 5.5/11.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 6.0/11.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 6.6/11.0 MB 2.8 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 7.1/11.0 MB 2.8 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.6/11.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ----------------------------- ---------- 8.1/11.0 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------- -------- 8.7/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.2/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.7/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.2/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  10.7/11.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 2.6 MB/s eta 0:00:00\n",
      "Downloading scipy-1.14.1-cp312-cp312-win_amd64.whl (44.5 MB)\n",
      "   ---------------------------------------- 0.0/44.5 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/44.5 MB 2.4 MB/s eta 0:00:19\n",
      "    --------------------------------------- 1.0/44.5 MB 2.8 MB/s eta 0:00:16\n",
      "   - -------------------------------------- 1.6/44.5 MB 2.8 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.4/44.5 MB 2.8 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 2.6/44.5 MB 2.7 MB/s eta 0:00:16\n",
      "   -- ------------------------------------- 3.1/44.5 MB 2.6 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 3.7/44.5 MB 2.5 MB/s eta 0:00:17\n",
      "   --- ------------------------------------ 4.2/44.5 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 4.7/44.5 MB 2.5 MB/s eta 0:00:16\n",
      "   ---- ----------------------------------- 5.5/44.5 MB 2.6 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 6.0/44.5 MB 2.6 MB/s eta 0:00:16\n",
      "   ----- ---------------------------------- 6.6/44.5 MB 2.6 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 7.1/44.5 MB 2.6 MB/s eta 0:00:15\n",
      "   ------ --------------------------------- 7.6/44.5 MB 2.6 MB/s eta 0:00:15\n",
      "   ------- -------------------------------- 8.1/44.5 MB 2.6 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 8.9/44.5 MB 2.6 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 9.4/44.5 MB 2.6 MB/s eta 0:00:14\n",
      "   -------- ------------------------------- 10.0/44.5 MB 2.6 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 10.5/44.5 MB 2.6 MB/s eta 0:00:14\n",
      "   --------- ------------------------------ 11.0/44.5 MB 2.6 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 11.3/44.5 MB 2.6 MB/s eta 0:00:13\n",
      "   ---------- ----------------------------- 12.1/44.5 MB 2.6 MB/s eta 0:00:13\n",
      "   ----------- ---------------------------- 12.6/44.5 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------ --------------------------- 13.4/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 13.9/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   ------------ --------------------------- 14.4/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 14.9/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   ------------- -------------------------- 15.5/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 16.0/44.5 MB 2.6 MB/s eta 0:00:12\n",
      "   -------------- ------------------------- 16.5/44.5 MB 2.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 17.0/44.5 MB 2.6 MB/s eta 0:00:11\n",
      "   --------------- ------------------------ 17.6/44.5 MB 2.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 18.1/44.5 MB 2.6 MB/s eta 0:00:11\n",
      "   ---------------- ----------------------- 18.6/44.5 MB 2.6 MB/s eta 0:00:11\n",
      "   ----------------- ---------------------- 19.1/44.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ----------------- ---------------------- 19.7/44.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 20.2/44.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------ --------------------- 20.7/44.5 MB 2.6 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 21.2/44.5 MB 2.5 MB/s eta 0:00:10\n",
      "   ------------------- -------------------- 21.8/44.5 MB 2.6 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 22.3/44.5 MB 2.5 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 22.5/44.5 MB 2.6 MB/s eta 0:00:09\n",
      "   -------------------- ------------------- 23.1/44.5 MB 2.5 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 23.9/44.5 MB 2.5 MB/s eta 0:00:09\n",
      "   --------------------- ------------------ 24.4/44.5 MB 2.5 MB/s eta 0:00:08\n",
      "   ---------------------- ----------------- 24.9/44.5 MB 2.5 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 25.7/44.5 MB 2.6 MB/s eta 0:00:08\n",
      "   ----------------------- ---------------- 26.2/44.5 MB 2.5 MB/s eta 0:00:08\n",
      "   ------------------------ --------------- 26.7/44.5 MB 2.5 MB/s eta 0:00:07\n",
      "   ------------------------ --------------- 27.3/44.5 MB 2.6 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 28.0/44.5 MB 2.6 MB/s eta 0:00:07\n",
      "   ------------------------- -------------- 28.6/44.5 MB 2.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 29.1/44.5 MB 2.6 MB/s eta 0:00:07\n",
      "   -------------------------- ------------- 29.6/44.5 MB 2.6 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 30.1/44.5 MB 2.6 MB/s eta 0:00:06\n",
      "   --------------------------- ------------ 30.7/44.5 MB 2.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 31.2/44.5 MB 2.6 MB/s eta 0:00:06\n",
      "   ---------------------------- ----------- 32.0/44.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ----------------------------- ---------- 32.8/44.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 33.6/44.5 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------ --------- 34.3/44.5 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 35.1/44.5 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 35.9/44.5 MB 2.7 MB/s eta 0:00:04\n",
      "   -------------------------------- ------- 36.7/44.5 MB 2.7 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 37.5/44.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 38.0/44.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 38.8/44.5 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 39.3/44.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 40.1/44.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 41.2/44.5 MB 2.7 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 41.9/44.5 MB 2.7 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 42.7/44.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  43.5/44.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------  44.3/44.5 MB 2.8 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 44.5/44.5 MB 2.8 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, scikit-learn\n",
      "Successfully installed scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: C:\\Users\\mindf\\AppData\\Local\\Microsoft\\WindowsApps\\PythonSoftwareFoundation.Python.3.12_qbz5n2kfra8p0\\python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Steps in BoW:\n",
    "\n",
    "Tokenization: Split the text into individual words (tokens).\n",
    "\n",
    "Vocabulary Creation: Create a list of all unique words in the entire corpus.\n",
    "\n",
    "Word Frequency: Count the frequency of each word in each document.\n",
    "\n",
    "Vector Representation: Each document is represented as a vector of word frequencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:\n",
    "\n",
    "Let’s consider two documents:\n",
    "\n",
    "Document 1: \"I love programming in Python.\"\n",
    "\n",
    "Document 2: \"Python programming is fun.\"\n",
    "\n",
    "We’ll represent these documents using the Bag of Words model and then create a matrix where each row corresponds to a document, and each column corresponds to a word from the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['fun' 'in' 'is' 'love' 'programming' 'python']\n",
      "Bag of Words Matrix:\n",
      " [[0 1 0 1 1 1]\n",
      " [1 0 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Define the documents\n",
    "documents = [\"I love programming in Python.\", \"Python programming is fun.\"]\n",
    "\n",
    "# Create a CountVectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the documents into a Bag of Words representation\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the vocabulary (the unique words)\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the matrix to an array to see the word counts\n",
    "bow_matrix = X.toarray()\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Bag of Words Matrix:\\n\", bow_matrix)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 1: [0, 1, 0, 1, 1, 1] — This means \"Document 1\" has:\n",
    "0 occurrences of \"fun\"\n",
    "1 occurrence of \"in\"\n",
    "0 occurrences of \"is\"\n",
    "1 occurrence of \"love\"\n",
    "1 occurrence of \"programming\"\n",
    "1 occurrence of \"python\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is also a type of Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
